---
layout: post
title: Mixture of Experts Models in Deep Learning
date: 2024-08-01 21:01:00
description: A simple introduction to Mixture of Experts Models in Deep Learning
tags: formatting videos
categories: Learnings
published: true
---

Introduction to Mixture of Experts (MoE) Models in Deep Learning
Mixture of Experts (MoE) is a neural network architecture designed to dynamically allocate computational resources by using multiple expert subnetworks, each specializing in different aspects of a problem. The core idea behind MoE is to improve efficiency and scalability by selectively activating only a subset of the available experts for each input, rather than using a single large model for all computations.

Here is a simple video on You Tube:
[A simple introduction to Mixture of Experts Models in Deep Learning](https://www.youtube.com/watch?v=OtMD1U7HPZs&t=33s) 

<div class="row mt-3">
    <div class="col-12 mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/MoE.jpg" class="img-fluid rounded z-depth-1" zoomable=false %}
    </div>
</div>



